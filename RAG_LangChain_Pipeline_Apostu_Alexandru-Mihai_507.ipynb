{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744e6ad2-1653-495a-ae1b-4b1dfb98d202",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a528beda-49cf-452d-83bd-a3644122556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyz\\anaconda3\\envs\\dizertatie\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import time\n",
    "\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI, Body\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import requests\n",
    "import threading\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc628a5a-b743-4d27-b3fd-64174c386636",
   "metadata": {},
   "source": [
    "# Setting up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab56cc3-679c-476b-9bda-624488911c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'Set your OpenAI API KEY Here'\n",
    "os.environ['PINECONE_API_KEY'] = 'Set your Pinecone API KEY Here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994ea74-f8ab-4111-9d70-9032ad994c9f",
   "metadata": {},
   "source": [
    "# Embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbfc4f3-84cb-46ee-a30c-8c45c96b0936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyz\\anaconda3\\envs\\dizertatie\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model = model_name,\n",
    "    openai_api_key = OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7624df-14b1-4546-853c-971feb385ac2",
   "metadata": {},
   "source": [
    "# Connecting to Pinecone DB Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba47fdd-ec86-4ba1-bb71-3b9a9253c3f2",
   "metadata": {},
   "source": [
    "- If you do not have an index to connect to, go to the end of the notebook and create/populate one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfdceb5-416d-4979-9ddf-037cb68b830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"celestial-7k-db\"\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()\n",
    "\n",
    "vectorstore = PineconeVectorStore.from_existing_index(index_name, embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2705a-9e0a-4f29-ac81-f6146a7d882f",
   "metadata": {},
   "source": [
    "# Setting up the Retrieval QA Bot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e8e906-c9ff-4751-80d1-954ebca7ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyz\\anaconda3\\envs\\dizertatie\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "class RetrievalQABot():\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key = OPENAI_API_KEY,\n",
    "        model_name = 'gpt-4o',\n",
    "        #streaming = True,        # Disabled streaming because get_openai_callback() (Testing method) does not work for prompts that are streamed\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        temperature = 0.5\n",
    "    )\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are a very knowledgeable NPC assistant in a video game. Players will come to you and ask questions regarding the gameplay.\n",
    "    Try to retrieve the answer from the context alone and limit your answer to these documents. \n",
    "    If you do not know the answer to their question, just say you don't know. \n",
    "    Keep the answer within maximum 2-3 sentences and concise.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Given the following conversation, answer the question.\n",
    "\n",
    "    Chat History: {chat_history}\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"context\", \"question\"],\n",
    "    template = prompt_template,\n",
    "    )\n",
    "\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "                                    llm = llm,\n",
    "                                    chain_type = 'stuff',\n",
    "                                    retriever = MultiQueryRetriever.from_llm(               # Each query will be the equivalent of 3 queries\n",
    "                                                retriever = vectorstore.as_retriever(),\n",
    "                                                llm=llm),\n",
    "                                    #verbose = True,\n",
    "                                    chain_type_kwargs = {\n",
    "                                    #\"verbose\": True,\n",
    "                                    \"prompt\": prompt,\n",
    "                                    \"memory\": ConversationBufferWindowMemory(\n",
    "                                        llm = llm,\n",
    "                                        k = 6,\n",
    "                                        memory_key = \"chat_history\",\n",
    "                                        input_key = \"question\"),\n",
    "                                    }\n",
    "    )\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        result = self.rag_chain.invoke(query)\n",
    "        return f\"Final Answer: {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92a360-c02d-41f2-bea3-b576db1a8eee",
   "metadata": {},
   "source": [
    "# Initializing the bot and querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce0acf17-9ed7-4df9-965a-111bf42f3d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask me anything:  What game is this ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: {'query': 'What game is this ?', 'result': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "bot = RetrievalQABot()\n",
    "user_input = input(\"Ask me anything: \")\n",
    "result = bot.process_query(user_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ef5cbd06-9d06-4b2f-bce2-d235e1ed5a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are some good looking biomes ?',\n",
       " 'result': 'Some good-looking biomes include the dense forested biome with lush green landscapes and large jungle trees, and the swamp biome characterized by shallow pools of green water with floating lily pads, trees covered with vines, and abundant mushrooms and sugar canes.'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = RetrievalQABot()\n",
    "bot.rag_chain.invoke(\"What are some good looking biomes ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8924a-d098-4edd-a41a-59ad9233f22c",
   "metadata": {},
   "source": [
    "# Building the chatbot API using FASTAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62bb6a1c-028e-44d0-8a19-fca203049bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to run FastAPI within Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "chatbot = RetrievalQABot()  \n",
    "\n",
    "class Query(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(query: Query):\n",
    "    response = chatbot.process_query(query.text)  \n",
    "    return {\"response\": response}\n",
    "\n",
    "@app.get(\"/status\")\n",
    "async def health():\n",
    "    return {\"status\": \"Application is functional !\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf755fb-7fc0-40ad-8c85-db6ecfefb59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [42792]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:56552 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56557 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56558 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56559 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56560 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56561 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56562 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56563 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56564 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56565 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56566 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56567 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56568 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56569 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56570 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56571 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56572 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56573 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56574 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56575 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56576 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56577 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56578 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56579 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56580 - \"POST /chat HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_api():\n",
    "    config = uvicorn.Config(app=app, host=\"localhost\", port=8000, log_level=\"info\", reload=True)\n",
    "    server = uvicorn.Server(config)\n",
    "    server.run()\n",
    "\n",
    "# Function to stop the server\n",
    "def stop_server():\n",
    "    # You would need a way to reference and stop the running server.\n",
    "    pass\n",
    "\n",
    "# Run the server in a thread\n",
    "thread = threading.Thread(target=run_api)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d8e81-3907-490d-be9c-5561029d1582",
   "metadata": {},
   "source": [
    "# Testing POST Requests on the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921b6c0-2d06-4445-9615-b0ab8d55ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post('http://localhost:8000/chat', json={\"text\": \"What is the best tool?\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c5aa033-2793-437f-b8ca-446ee5d1b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Final Answer: {\\'query\\': \"Imagine me a story about Techton\\'s past\", \\'result\\': \"Techton\\'s past is shrouded in mystery, with rumors of a dark past as a warrior before he turned to blacksmithing. Some say he once wielded a legendary sword that held the power to control the elements, but he now uses his skills to forge weapons for the greater good.\"}'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post('http://localhost:8000/chat', json={\"text\": \"Imagine me a story about Techton's past\"})\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "507a6078-e1da-40b1-9127-76fac1b1bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Final Answer: {'query': 'What is the Crystal Nexus?', 'result': 'The Crystal of Nexus is a powerful artifact that maintains the balance between the realms in Eldoria. It is sought after by dark forces seeking to manipulate the convergence for their own nefarious purposes.'}\"}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post('http://localhost:8000/chat', json={\"text\": \"What is the Crystal Nexus?\"})\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bb9d4-dc3f-4c39-98c2-538f93c2a2c9",
   "metadata": {},
   "source": [
    "# Index Creation and Populating a DB - ONLY If needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314ad8c-dd0f-4209-8aff-dbe3a2a3f01a",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700d90e4-3123-413a-bbfa-6fe6cf41cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('CelestialAscension_7k.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 20)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a903e0-796f-4a9b-b81f-19ad9fdec4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the document\n",
    "clean_kb = [doc.page_content for doc in docs if doc.page_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184d507d-f422-45fd-b1e6-9ba590cb2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model = model_name,\n",
    "    openai_api_key = OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94fcec-e712-4a75-a333-d20c08e8c9b9",
   "metadata": {},
   "source": [
    "## Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ac6b74-d016-4bfb-b651-8c9018453b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652c6833-f2ba-4499-82dd-c59f84e900b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d66099-2cad-42c4-879e-cfc0b80246db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"celestial-ascension-7k-db\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension = 1536,  # dimensionality of ada-002\n",
    "        metric = 'dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54d2c0-764b-49f7-8118-edf159889226",
   "metadata": {},
   "source": [
    "## Index population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd82e449-ba04-453d-a493-94cbab87b994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c579e795bdb14a7196935203dab988f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 100}},\n",
      " 'total_vector_count': 100}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming clean_kb is already a list of cleaned text strings\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(clean_kb), batch_size)):\n",
    "    i_end = min(len(clean_kb), i + batch_size)\n",
    "    batch = clean_kb[i:i_end]\n",
    "\n",
    "    ids = [f'doc_{j}' for j in range(i, i_end)]\n",
    "\n",
    "    try:\n",
    "        embeds = embed.embed_documents(batch)\n",
    "\n",
    "        metadata = [{'text': text} for text in batch]\n",
    "\n",
    "        upsert_data = zip(ids, embeds, metadata)\n",
    "\n",
    "        index.upsert(vectors=upsert_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed or upsert documents: {e}\")\n",
    "\n",
    "print(index.describe_index_stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
